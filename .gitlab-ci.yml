image: gitlab.green-rabbit.net:5050/kimata/local-ubuntu:250706_043ea01d

variables:
    UV_LINK_MODE: copy
    UV_CACHE_DIR: .uv-cache

stages:
    - generate-tag
    - build
    - test
    - tag-latest
    - deploy
    - renovate

generate-tag:
    stage: generate-tag

    script:
        - echo "TAG=$(date +%y%m%d)_${CI_COMMIT_SHORT_SHA}" > tag.env

    artifacts:
        reports:
            dotenv: tag.env
        expire_in: 1 hour

build-vue:
    stage: build

    needs: []

    image: node:22.15
    script:
        - npm ci --cache .npm --prefer-offline
        - npm run build
    artifacts:
        paths:
            - dist/
    cache:
        key: "${CI_JOB_NAME}"
        paths:
            - .npm/

build-image:
    stage: build

    needs:
        - generate-tag
        - build-vue

    variables:
        BUILDER: builder
        BUILD_KIT_IMAGE: gitlab.green-rabbit.net:5050/kimata/local-buildkit:250705_7cc0d1c

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        - 'echo "Building: ${CI_REGISTRY_IMAGE}:${TAG}"'

        - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.green-rabbit.net/kimata/hems-config.git
        - mv hems-config/rasp-shutter.yaml config.yaml

        - |
            docker buildx create \
                --name ${BUILDER} \
                --driver-opt image=${BUILD_KIT_IMAGE} \
                --use \
                --config /etc/buildkitd.toml

        - docker buildx use ${BUILDER}
        - docker buildx inspect --bootstrap
        - >
            docker buildx build --provenance=false --progress=plain --platform linux/amd64,linux/arm64/v8
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:cache
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:latest
            --cache-to type=inline --cache-to type=registry,ref=${CI_REGISTRY_IMAGE}:cache,mode=max
            --build-arg IMAGE_BUILD_DATE=$(date --iso-8601=seconds)
            --tag ${CI_REGISTRY_IMAGE}:${TAG} --push .

test-prepare:
    stage: build

    needs: []

    script:
        - uv sync --locked --no-editable

    artifacts:
        paths:
            - ${UV_CACHE_DIR}
        expire_in: 1 hour

    cache:
        - key:
              files:
                  - uv.lock
          paths:
              - ${UV_CACHE_DIR}

test-walk-through:
    stage: test

    needs:
        - test-prepare

    script:
        - uv run pytest --numprocesses=auto --junit-xml=tests/evidence/junit-report.xml tests/test_basic.py

    artifacts:
        when: always
        paths:
            - tests/evidence/**
        reports:
            junit: tests/evidence/junit-report.xml

test-playwright:
    stage: test

    needs:
        - build-vue
        - test-prepare

    script:
        - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.green-rabbit.net/kimata/hems-config.git
        - mv hems-config/rasp-shutter.yaml config.yaml

        - uv run playwright install --with-deps chromium
        - uv run python ./flask/src/app.py -d 2>&1 > flask_log.txt &
        - >
            uv run pytest --tracing on --output tests/evidence/playwright
            --junit-xml=tests/evidence/playwright-junit.xml
            tests/test_playwright.py

    cache:
        key: playwright-cache
        paths:
            - .cache/ms-playwright/

    artifacts:
        when: always
        paths:
            - flask_log.txt
            - tests/evidence/**
        reports:
            junit: tests/evidence/playwright-junit.xml

.test-playwright-docker-base:
    stage: test

    # 同一ジョブの並行実行を防止してリソース競合を回避
    resource_group: ${CI_JOB_NAME}

    needs:
        - generate-tag
        - build-image
        - test-prepare

    variables:
        CONFIG_ARGS: "-d"

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        # クリーンアップ: 古いコンテナを停止
        - >
            for id in $(docker ps --filter "label=job=${CI_PROJECT_NAME}-${CI_JOB_NAME}" --format "{{.ID}}"); do
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$id")
                started_epoch=$(date --date="$started_at" +%s)
                now_epoch=$(date +%s)
                diff=$(( now_epoch - started_epoch ))

                if [ "$diff" -ge 600 ]; then
                    echo "Stopping container $id (running for $diff seconds)"
                    docker stop -t 10 "$id"
                fi
            done

        # クリーンアップ: 古いネットワークを削除
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network 2>/dev/null || true

        # 専用ネットワーク作成（プロジェクト固有）
        - docker network create ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network

        # テスト用に一時的なポート公開でコンテナ起動
        - >
            docker run --rm --detach=true
            --name ${CI_JOB_NAME}-${CI_JOB_ID} --label job=${CI_PROJECT_NAME}-${CI_JOB_NAME}
            --network ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
            --publish :5000
            ${CI_REGISTRY_IMAGE}:${TAG} ./flask/src/app.py ${CONFIG_ARGS}

        - echo "Container started, checking status..."
        - docker ps | grep ${CI_PROJECT_NAME}-${CI_JOB_NAME}

        - uv run playwright install --with-deps chromium

        # アプリケーションの起動を待つ（公開ポート経由）
        - >
            APP_PORT=$(docker port ${CI_PROJECT_NAME}-${CI_JOB_NAME} 5000 | cut -d: -f2)
        - >
            APP_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            echo "Testing at http://${APP_HOST}:${APP_PORT}/rasp-shutter/"
        - >
            timeout 90 bash -c "until curl -f --connect-timeout 5 --max-time 10
            http://${APP_HOST}:${APP_PORT}/rasp-shutter/ > /dev/null 2>&1;
            do echo 'Waiting for app to start...'; sleep 3; done" || ACCESS_FAILED=1

        - |
            if [ "$ACCESS_FAILED" = "1" ]; then
                echo "Failed to access App"
                docker logs ${CI_PROJECT_NAME}-${CI_JOB_NAME} > flask_log.txt
                docker stop ${CI_PROJECT_NAME}-${CI_JOB_NAME} || true
                docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network || true
                exit 1
            fi

        # Playwrightテストを実行（ホストからポート経由で接続）
        - >
            APP_PORT=$(docker port ${CI_PROJECT_NAME}-${CI_JOB_NAME} 5000 | cut -d: -f2)
        - >
            APP_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            echo "Testing at http://${APP_HOST}:${APP_PORT}/rasp-shutter/"
        - >
            uv run pytest --tracing on --output tests/evidence/playwright
            --junit-xml=tests/evidence/playwright-docker-junit.xml
            tests/test_playwright.py --host ${APP_HOST} --port ${APP_PORT}

        # ログ取得とクリーンアップ
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network || true

    cache:
        key: playwright-cache
        paths:
            - .cache/ms-playwright/

    artifacts:
        when: always
        paths:
            - flask_log_*.txt
            - tests/evidence/**
        reports:
            junit: tests/evidence/playwright-docker-junit.xml

test-playwright-docker:
    extends: .test-playwright-docker-base
    variables:
        JOB_TYPE: docker
        CONFIG_ARGS: "-d"

test-playwright-docker-demo:
    extends: .test-playwright-docker-base
    variables:
        JOB_TYPE: docker-demo
        CONFIG_ARGS: "-d -c config.example.yaml"

test-healthz-docker:
    stage: test

    # 同一ジョブの並行実行を防止してリソース競合を回避
    resource_group: ${CI_JOB_NAME}

    needs:
        - generate-tag
        - build-image

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        # クリーンアップ: 古いコンテナを停止
        - >
            for id in $(docker ps --filter "label=job=${CI_PROJECT_NAME}-${CI_JOB_NAME}" --format "{{.ID}}"); do
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$id")
                started_epoch=$(date --date="$started_at" +%s)
                now_epoch=$(date +%s)
                diff=$(( now_epoch - started_epoch ))

                if [ "$diff" -ge 600 ]; then
                    echo "Stopping container $id (running for $diff seconds)"
                    docker stop -t 10 "$id"
                fi
            done

        # クリーンアップ: 古いネットワークを削除
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network 2>/dev/null || true

        # 専用ネットワーク作成（プロジェクト固有）
        - docker network create ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network

        # ポート公開でコンテナ起動
        - >
            docker run --rm --detach=true
            --name ${CI_JOB_NAME}-${CI_JOB_ID} --label job=${CI_PROJECT_NAME}-${CI_JOB_NAME}
            --network ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network
            --publish :5000
            ${CI_REGISTRY_IMAGE}:${TAG} ./flask/src/app.py -d

        - echo "Container started, checking status..."
        - docker ps | grep ${CI_PROJECT_NAME}-${CI_JOB_NAME}

        # アプリケーションの起動を待つ（公開ポート経由）
        - >
            APP_PORT=$(docker port ${CI_JOB_NAME}-${CI_JOB_ID} 5000 | cut -d: -f2)
        - >
            APP_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            echo "Testing at http://${APP_HOST}:${APP_PORT}/rasp-shutter/"
        - >
            timeout 90 bash -c "until curl -f --connect-timeout 5 --max-time 10
            http://${APP_HOST}:${APP_PORT}/rasp-shutter/ > /dev/null 2>&1;
            do echo 'Waiting for app to start...'; sleep 3; done" || ACCESS_FAILED=1

        - |
            if [ "$ACCESS_FAILED" = "1" ]; then
                echo "Failed to access App"
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
                docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
                docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network || true
                exit 1
            fi

        # ヘルスチェック実行
        - echo "App is ready, running health check..."
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
        - docker exec ${CI_JOB_NAME}-${CI_JOB_ID} ./flask/src/healthz.py

        # クリーンアップ
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
        - docker network rm ${CI_PROJECT_NAME}-${CI_JOB_NAME}-network || true
    artifacts:
        when: always
        paths:
            - flask_log.txt

pages:
    stage: test
    needs:
        - test-walk-through

    script:
        - mkdir -p public/
        - cp -r tests/evidence/* public/

    artifacts:
        paths:
            - public

    only:
        - master

tag-latest:
    stage: tag-latest

    needs:
        - generate-tag
        - build-image
        - job: test-walk-through
          artifacts: false
        - job: test-playwright
          artifacts: false
        - job: test-playwright-docker
          artifacts: false
        - job: test-playwright-docker-demo
          artifacts: false
        - job: test-healthz-docker
          artifacts: false

    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

    script:
        - 'echo "Tagging multi-arch image ${CI_REGISTRY_IMAGE}:${TAG} as latest"'
        - docker buildx imagetools create -t ${CI_REGISTRY_IMAGE}:latest ${CI_REGISTRY_IMAGE}:${TAG}

    rules:
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

deploy-webapp:
    stage: deploy

    needs:
        - generate-tag
        - build-image
        - job: test-walk-through
          artifacts: false
        - job: test-playwright
          artifacts: false
        - job: test-playwright-docker
          artifacts: false
        - job: test-playwright-docker-demo
          artifacts: false
        - job: test-healthz-docker
          artifacts: false

    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-kubectl:250715_c88f3965

    script:
        - 'IMAGE="${CI_REGISTRY_IMAGE}:${TAG}"'
        - 'echo "Deploying image: $IMAGE"'

        - kubectl config get-contexts
        - kubectl config use-context kimata/rasp-shutter:pod-rollout

        - kubectl -n hems set image deployment/rasp-shutter rasp-shutter=${IMAGE}
        - kubectl -n hems set image deployment/rasp-shutter-demo rasp-shutter-demo=${IMAGE}

        - kubectl -n hems rollout status deployment/rasp-shutter --timeout=300s
        - kubectl -n hems rollout status deployment/rasp-shutter-demo --timeout=300s

        - 'echo "✓ All deployments updated successfully to $TAG"'

    rules:
        # NOTE: 自動実行ではデプロイしない
        - if: $CI_PIPELINE_SOURCE == "schedule"
          when: never
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

renovate:
    stage: renovate
    dependencies: []
    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-renovate:250715_3b8866ff

    script:
        - renovate --platform gitlab --token ${RENOVATE_TOKEN} --endpoint ${CI_SERVER_URL}/api/v4 ${CI_PROJECT_PATH}
    rules:
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - if: $CI_PIPELINE_SOURCE == "schedule"
        - changes:
              - renovate.json
