image: gitlab.green-rabbit.net:5050/kimata/local-ubuntu:260104_3efe597c

variables:
    UV_LINK_MODE: copy
    UV_CACHE_DIR: .uv-cache
    UV_EXTRA_INDEX_URL: "http://proxy:3141/simple/"
    UV_INSECURE_HOST: "proxy"

default:
    before_script:
        - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

stages:
    - generate-tag
    - build
    - test
    - tag-latest
    - deploy
    - renovate

generate-tag:
    stage: generate-tag

    script:
        - echo "TAG=$(date +%y%m%d)_${CI_COMMIT_SHORT_SHA}" > tag.env

    artifacts:
        reports:
            dotenv: tag.env
        expire_in: 1 hour

build-vue:
    stage: build

    needs: []

    image: node:22.15

    before_script: []

    script:
        - cd frontend && npm ci --cache .npm --prefer-offline --no-audit --no-fund && npm run build
    artifacts:
        paths:
            - frontend/dist/
    cache:
        - key:
              files:
                  - frontend/package-lock.json
          fallback_keys:
              - npm-cache-${CI_PROJECT_ID}
          paths:
              - frontend/.npm/

build-image:
    stage: build

    needs:
        - generate-tag
        - build-vue

    variables:
        BUILDER: builder-${CI_PROJECT_NAME}

    script:
        - 'echo "Building: ${CI_REGISTRY_IMAGE}:${TAG}"'

        - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.green-rabbit.net/kimata/hems-config.git
        - mv hems-config/rasp-shutter.yaml config.yaml

        - |
            docker buildx create \
                --name ${BUILDER} \
                --use \
                --config /etc/buildkitd.toml

        - docker buildx use ${BUILDER}
        - docker buildx inspect --bootstrap
        - >
            docker buildx build --provenance=false --progress=plain --platform linux/amd64,linux/arm64/v8
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:cache
            --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}:latest
            --cache-to type=inline --cache-to type=registry,ref=${CI_REGISTRY_IMAGE}:cache,mode=max
            --build-arg IMAGE_BUILD_DATE=$(date --iso-8601=seconds)
            --tag ${CI_REGISTRY_IMAGE}:${TAG} --push .
    # after_script:
    #     - docker buildx rm ${BUILDER} || true

test-pytest:
    stage: test

    needs: []

    before_script:
        - uv sync --locked --no-editable

    script:
        - >
            uv run pytest tests/unit/ tests/integration/
            --junit-xml=reports/junit-report.xml --maxfail=1 --dist=loadgroup

    coverage: '/TOTAL.*\s+(\d+%)/'

    cache:
        - key:
              files:
                  - uv.lock
          fallback_keys:
              - uv-cache-${CI_PROJECT_ID}
          paths:
              - ${UV_CACHE_DIR}

    artifacts:
        when: always
        paths:
            - reports/**
            - htmlcov/**
        reports:
            junit: reports/junit-report.xml
            coverage_report:
                coverage_format: cobertura
                path: reports/coverage.xml

test-playwright:
    stage: test

    needs:
        - build-vue

    before_script:
        - uv sync --locked --no-editable

    script:
        - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.green-rabbit.net/kimata/hems-config.git
        - mv hems-config/rasp-shutter.yaml config.yaml

        - uv run playwright install --with-deps chromium
        - env TEST=true uv run python ./src/app.py -d 2>&1 > flask_log.txt &
        - >
            uv run pytest tests/e2e/ --no-cov -n 0
            --junit-xml=reports/playwright-junit.xml

    cache:
        - key:
              files:
                  - uv.lock
          fallback_keys:
              - uv-cache-${CI_PROJECT_ID}
          paths:
              - ${UV_CACHE_DIR}
        - key: playwright-${CI_PROJECT_ID}
          paths:
              - .cache/ms-playwright/

    artifacts:
        when: always
        paths:
            - flask_log.txt
        reports:
            junit: reports/playwright-junit.xml

.test-playwright-docker-base:
    stage: test

    # 同一ジョブの並行実行を防止してリソース競合を回避
    resource_group: ${CI_JOB_NAME}

    needs:
        - generate-tag
        - build-image

    variables:
        CONFIG_ARGS: "-d"

    before_script:
        - uv sync --locked --no-editable

    script:
        # クリーンアップ: 古いコンテナを停止
        - |
            for id in $(docker ps --filter "label=job=${CI_PROJECT_NAME}-${CI_JOB_NAME}" --format "{{.ID}}"); do
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$id")
                started_epoch=$(date --date="$started_at" +%s)
                now_epoch=$(date +%s)
                diff=$(( now_epoch - started_epoch ))

                if [ "$diff" -ge 600 ]; then
                    echo "Stopping container $id (running for $diff seconds)"
                    docker stop -t 10 "$id" || true
                fi
            done

        - DOCKER_NETWORK=${CI_PROJECT_NAME}-${CI_JOB_NAME}-${CI_JOB_ID}-network

        # 専用ネットワーク作成（プロジェクト固有）
        - docker network create ${DOCKER_NETWORK}

        # テスト用に一時的なポート公開でコンテナ起動
        - >
            docker run --rm --detach=true
            --name ${CI_JOB_NAME}-${CI_JOB_ID} --label job=${CI_PROJECT_NAME}-${CI_JOB_NAME}
            --network ${DOCKER_NETWORK} --publish :5000 --env TEST=true
            ${CI_REGISTRY_IMAGE}:${TAG}
            ./src/app.py ${CONFIG_ARGS}

        - echo "Container started, checking status..."
        - docker ps -a --filter name=${CI_JOB_NAME}-${CI_JOB_ID}

        - uv run playwright install --with-deps chromium

        # アプリケーションの起動を待つ（公開ポート経由）
        - >
            APP_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            APP_PORT=$(docker port ${CI_JOB_NAME}-${CI_JOB_ID} 5000 | cut -d: -f2)
        - >
            echo "Testing at http://${APP_HOST}:${APP_PORT}/rasp-shutter/"
        - >
            timeout 90 bash -c "until curl -f --connect-timeout 5 --max-time 10
            http://${APP_HOST}:${APP_PORT}/rasp-shutter/ > /dev/null 2>&1;
            do echo 'Waiting for app to start...'; sleep 3; done" || ACCESS_FAILED=1

        - |
            if [ "$ACCESS_FAILED" = "1" ]; then
                echo "Failed to access App"
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
                docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
                docker network rm ${DOCKER_NETWORK} || true
                exit 1
            fi

        - >
            uv run pytest tests/e2e/ --no-cov -n 0
            --junit-xml=reports/playwright-docker-junit.xml
            --host ${APP_HOST} --port ${APP_PORT}

        # ログ取得とクリーンアップ
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
        - docker network rm ${DOCKER_NETWORK} || true

    cache:
        - key:
              files:
                  - uv.lock
          fallback_keys:
              - uv-cache-${CI_PROJECT_ID}
          paths:
              - ${UV_CACHE_DIR}
        - key: playwright-${CI_PROJECT_ID}
          paths:
              - .cache/ms-playwright/

    artifacts:
        when: always
        paths:
            - flask_log.txt
        reports:
            junit: reports/playwright-docker-junit.xml

test-playwright-docker:
    extends: .test-playwright-docker-base
    variables:
        JOB_TYPE: docker
        CONFIG_ARGS: "-d"

test-playwright-docker-demo:
    extends: .test-playwright-docker-base
    variables:
        JOB_TYPE: docker-demo
        CONFIG_ARGS: "-d -c config.example.yaml"

test-healthz-docker:
    stage: test

    # 同一ジョブの並行実行を防止してリソース競合を回避
    resource_group: ${CI_JOB_NAME}

    needs:
        - generate-tag
        - build-image

    script:
        # クリーンアップ: 古いコンテナを停止
        - |
            for id in $(docker ps --filter "label=job=${CI_PROJECT_NAME}-${CI_JOB_NAME}" --format "{{.ID}}"); do
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$id")
                started_epoch=$(date --date="$started_at" +%s)
                now_epoch=$(date +%s)
                diff=$(( now_epoch - started_epoch ))

                if [ "$diff" -ge 600 ]; then
                    echo "Stopping container $id (running for $diff seconds)"
                    docker stop -t 10 "$id" || true
                fi
            done

        - DOCKER_NETWORK=${CI_PROJECT_NAME}-${CI_JOB_NAME}-${CI_JOB_ID}-network

        # 専用ネットワーク作成（プロジェクト固有）
        - docker network create ${DOCKER_NETWORK}

        # ポート公開でコンテナ起動
        - >
            docker run --rm --detach=true
            --name ${CI_JOB_NAME}-${CI_JOB_ID} --label job=${CI_PROJECT_NAME}-${CI_JOB_NAME}
            --network ${DOCKER_NETWORK} --publish :5000
            ${CI_REGISTRY_IMAGE}:${TAG} ./src/app.py -d

        - echo "Container started, checking status..."
        - docker ps -a --filter name=${CI_JOB_NAME}-${CI_JOB_ID}

        # アプリケーションの起動を待つ（公開ポート経由）
        - >
            APP_HOST=$(docker network inspect bridge --format="{{range .IPAM.Config}}{{.Gateway}}{{end}}")
        - >
            APP_PORT=$(docker port ${CI_JOB_NAME}-${CI_JOB_ID} 5000 | cut -d: -f2)
        - >
            echo "Testing at http://${APP_HOST}:${APP_PORT}/rasp-shutter/"
        - >
            timeout 90 bash -c "until curl -f --connect-timeout 5 --max-time 10
            http://${APP_HOST}:${APP_PORT}/rasp-shutter/ > /dev/null 2>&1;
            do echo 'Waiting for app to start...'; sleep 3; done" || ACCESS_FAILED=1

        - |
            if [ "$ACCESS_FAILED" = "1" ]; then
                echo "Failed to access App"
                docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
                docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
                docker network rm ${DOCKER_NETWORK} || true
                exit 1
            fi

        # ヘルスチェック実行
        - echo "App is ready, running health check..."
        - docker logs ${CI_JOB_NAME}-${CI_JOB_ID} > flask_log.txt
        - docker exec ${CI_JOB_NAME}-${CI_JOB_ID} ./src/healthz.py

        # クリーンアップ
        - docker stop ${CI_JOB_NAME}-${CI_JOB_ID} || true
        - docker network rm ${DOCKER_NETWORK} || true
    artifacts:
        when: always
        paths:
            - flask_log.txt

pages:
    stage: test
    needs:
        - test-pytest

    script:
        - mkdir -p public/
        - cp -r reports/* public/

    artifacts:
        paths:
            - public

    only:
        - master

tag-latest:
    stage: tag-latest

    needs:
        - generate-tag
        - build-image
        - job: test-pytest
          artifacts: false
        - job: test-playwright
          artifacts: false
        - job: test-playwright-docker
          artifacts: false
        - job: test-playwright-docker-demo
          artifacts: false
        - job: test-healthz-docker
          artifacts: false

    script:
        - 'echo "Tagging multi-arch image ${CI_REGISTRY_IMAGE}:${TAG} as latest"'
        - docker buildx imagetools create -t ${CI_REGISTRY_IMAGE}:latest ${CI_REGISTRY_IMAGE}:${TAG}

    rules:
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

deploy-webapp:
    stage: deploy

    needs:
        - generate-tag
        - build-image
        - job: test-pytest
          artifacts: false
        - job: test-playwright
          artifacts: false
        - job: test-playwright-docker
          artifacts: false
        - job: test-playwright-docker-demo
          artifacts: false
        - job: test-healthz-docker
          artifacts: false

    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-kubectl:250715_c88f3965

    before_script: []

    script:
        - 'IMAGE="${CI_REGISTRY_IMAGE}:${TAG}"'
        - 'echo "Deploying image: $IMAGE"'

        - kubectl config get-contexts
        - kubectl config use-context kimata/rasp-shutter:pod-rollout

        - kubectl -n hems set image deployment/rasp-shutter rasp-shutter=${IMAGE}
        - kubectl -n hems set image deployment/rasp-shutter-demo rasp-shutter-demo=${IMAGE}

        - kubectl -n hems rollout status deployment/rasp-shutter --timeout=300s
        - kubectl -n hems rollout status deployment/rasp-shutter-demo --timeout=300s

        - 'echo "✓ All deployments updated successfully to $TAG"'

    rules:
        # NOTE: 自動実行ではデプロイしない
        - if: $CI_PIPELINE_SOURCE == "schedule"
          when: never
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - when: on_success

renovate:
    stage: renovate
    dependencies: []
    image:
        name: gitlab.green-rabbit.net:5050/kimata/local-renovate:260104_3da7b77b

    before_script: []

    script:
        - renovate --platform gitlab --token ${RENOVATE_TOKEN} --endpoint ${CI_SERVER_URL}/api/v4 ${CI_PROJECT_PATH}
    rules:
        - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH
          when: never
        - if: $CI_PIPELINE_SOURCE == "schedule"
        - changes:
              - renovate.json
